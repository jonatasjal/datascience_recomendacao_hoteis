# -*- coding: utf-8 -*-
"""[Jonatas-Liberato]sistema-recomendação-hotéis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hGCHsHVp3B0DPLYWiWG4n9JMwPelPCl3

# **SISTEMA DE RECOMENDAÇÃO DE HOTÉIS**

A plataforma Booking nada mais é que um sistema intermediario entre você que está interessado em alugar acomodações.

-- 

**Como a própria empresa define seus serviços:** *"É importante lembrar que o acordo é feito diretamente entre você e o hóspede. Ao contrário de outros sites de reserva de acomodações, a Booking.com não é uma parte contratante na transação entre a sua propriedade e o hóspede. Desta forma, você tem controle total sobre as suas tarifas e disponibilidade e pode transmitir aos hóspedes mais transparência sobre as regras da sua propriedade, políticas e tudo o que oferece."*

--

**Site da empresa:** [AQUI](https://www.googleadservices.com/pagead/aclk?sa=L&ai=DChcSEwjt_Py0rIb6AhVMFNQBHRWABbcYABABGgJvYQ&ohost=www.google.com&cid=CAESauD2nUqsRaAPcex1NBVQHNvNdvCY5ScbYNEjCDXop4Ja4umsvZ83JN1JYHcPJdeHQAZTFpg6fJ3yhBxLnoJwgLImvzR2TOqn3VH8tnftMmp_wsPKpgn-Q8xC3Pee1I0n6qe4jrN63NITsK8&sig=AOD64_2eyKTRj0VvKr_VqTn2XfCcu3XR8A&q&adurl&ved=2ahUKEwj1wfG0rIb6AhWpqpUCHQY9AR8Q0Qx6BAgEEAE)

# Nosso objetivo

# O que será estudado

Dentre os aprendizados que serão obtidos com esse sistema, estão:
- Gráicos
- Técnicas de Análise Exploratória
- Processamento de Linguagem Natural (PLN)
- Máquinas preditivas

Para fins de estudo, construiremos um sistema de recomendação de hotéis, usando um dataset para a cidade de **Seatle**.

--

**Sua funcionalidade:**

Quando o usuário digitar o nome de um determinado hotel, o sistema traga outras opções de hotéis similares com esse primeiro. Com base em alguns características relevantes presentes nas recomendações.

**Exemplo:** mercado próximo, distância da praia, piscina, etc.

--

Isso é últil para que o usuário tenha uma experiência mais rica, com a possibilidade de conhecer novos locais. Muito semelhante à recomendação de séries do Netflix.

--

No dataset utilizado, teremos 3 colunas: 
- Nome da acomodação (name)
- Endereço (address)
- Recomendação inserida pelo usuário (desc)

**Dataset**: [AQUI](https://www.dropbox.com/s/v87muu24hudf548/Seattle_Hotels.csv?dl=0)

# 1 - Análise Exploratória

**Bibliotecas**
"""

import pandas as pd
import numpy as np
from nltk.corpus import stopwords
from sklearn.metrics.pairwise import linear_kernel
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import re
import random

import plotly.graph_objs as go
import cufflinks
pd.options.display.max_columns = 30
from IPython.core.interactiveshell import InteractiveShell
import plotly.figure_factory as ff
InteractiveShell.ast_node_interactivity = 'all'
from plotly.offline import iplot
cufflinks.go_offline()
cufflinks.set_config_file(world_readable=True, theme='solar')

"""**Dataset**"""

dataset = pd.read_csv('Seattle_Hotels.csv', encoding = 'latin-1')
dataset

dataset.head(8)

dataset.info()

print('Este dataset tem', len(dataset), 'registros de hotéis.')

# Uma função para mostrar as descrições dos hotéis
def print_desc(index):
  example = dataset[dataset.index == index][['desc', 'name']].values[0]
  if len(example) > 0:
    print(example[0])
    print('Name:', example[1])

print_desc(5)

print_desc(15)

"""**Aqui nosso primeiro tratamento para as palavras**

*Após contaros número de palavras, utilizamos todos os campos de descrições dos usuários para realizar um tratamento, eliminando pontuações, emojis e demais elementos que possam comprometer a análise e otimizando o processo.*

-- **OBS**: Nesse caso é utilizado o algoritmo de similaridade de coseno (o qual será abordado no meu perfil no Medim) para facilitar a busca por palavras próximas de acordo com a sua importância.

**Antes da remoção dos Stop Words**
"""

# Commented out IPython magic to ensure Python compatibility.
# Gerando uma visualização gráfica da distibuição das palavras
# %matplotlib inline
def get_top_n_words(corpus, n=None): # corpus junta todas as palavras em um único ambiente
    vec = CountVectorizer().fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(dataset['desc'], 20) # 20 primeiras palavras
dataset1 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
dataset1.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description before removing stop words')

"""**Depois da remoção das Stop Worrds**"""

def get_top_n_words(corpus, n=None):
    vec = CountVectorizer(stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_words(dataset['desc'], 20)
dataset2 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
dataset2.groupby('desc').sum()['count'].sort_values().iplot(kind='barh', yTitle='Count', linecolor='black', title='Top 20 words in hotel description after removing stop words')

"""**Criação do Bigrama (dois termos) -  ANTES da Remoção de Stop Words**"""

def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_bigram(dataset['desc'], 20)
dataset3 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
dataset3.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description before removing stop words')

"""**Criação do Bigrama (dois termos) -  APÓS Remoção de Stop Words**"""

def get_top_n_bigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(2, 2), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_bigram(df['desc'], 20)
df4 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
df4.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 bigrams in hotel description After removing stop words')

"""**Criação do Trigrama (três termos) -  ANTES da Remoção de Stop Words**"""

def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_trigram(dataset['desc'], 20)
dataset5 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
dataset5.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description before removing stop words')

"""**Criação do Trigrama (três termos) -  APÓS Remoção de Stop Words**"""

def get_top_n_trigram(corpus, n=None):
    vec = CountVectorizer(ngram_range=(3, 3), stop_words='english').fit(corpus)
    bag_of_words = vec.transform(corpus)
    sum_words = bag_of_words.sum(axis=0) 
    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]
    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)
    return words_freq[:n]
common_words = get_top_n_trigram(dataset['desc'], 20)
dataset6 = pd.DataFrame(common_words, columns = ['desc' , 'count'])
dataset6.groupby('desc').sum()['count'].sort_values(ascending=False).iplot(kind='bar', yTitle='Count', linecolor='black', title='Top 20 trigrams in hotel description after removing stop words')

"""# 2 - Pré-Processamento

**Contagem das palavras por descrição**
"""

dataset['word_count'] = dataset['desc'].apply(lambda x: len(str(x).split()))
dataset

desc_lengths = list(df['word_count'])

print("Number of descriptions:",len(desc_lengths),
      "\nAverage word count", np.average(desc_lengths),
      "\nMinimum word count", min(desc_lengths),
      "\nMaximum word count", max(desc_lengths))

# Commented out IPython magic to ensure Python compatibility.
# Visualização gráfica
# %matplotlib inline
df['word_count'].iplot(
    kind='hist',
    bins = 50,
    linecolor='black',
    xTitle='word count',
    yTitle='count',
    title='Contagem de Palavras na descrição dos Hotéis')

"""**Fazendo a limpeza das palavras.**

*Criamos uma função mara eliminar espaço, deixar tudo em letra minúscula, eliminar caracteretes inválidos, @, etc.*
"""

REPLACE_BY_SPACE_RE = re.compile('[/(){}\[\]\|@,;]') # espaço vazio, caracteres, @
BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]') # números e caracteres especiais
STOPWORDS = set(stopwords.words('english')) #

def clean_text(text):
    """
        text: o text é uma string
        
        return: o output será a versão limpa
    """
    text = text.lower() # lowercase text
    text = REPLACE_BY_SPACE_RE.sub(' ', text) 
    text = BAD_SYMBOLS_RE.sub('', text) 
    text = ' '.join(word for word in text.split() if word not in STOPWORDS)
    return text
    
df['desc_clean'] = df['desc'].apply(clean_text)

"""**Visualizando os dsdos após a eilinação das Stop Words**"""

def print_description(index):
    example = dataset[dataset.index == index][['desc_clean', 'name']].values[0]
    if len(example) > 0:
        print(example[0])
        print('Name:', example[1])
print_description(7)

def print_description(index):
    example = dataset[dataset.index == index][['desc_clean', 'name']].values[0]
    if len(example) > 0:
        print(example[0])
        print('Name:', example[1])
print_description(2)

# Visualizando um exemplo
print_description(50)

"""*Resetando os índices após o tratamento e para facilitar a pesquisa posterior.*"""

dataset.set_index('name', inplace = True)

"""# 3 - Máquina Preditiva"""

# TfidfVectorizer é uma função responsável por criar: Unigrama, Bigrama e Trigrama definido em ngram_range()
tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(df['desc_clean'])
cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix) # aplicando ao algoritmo atraés do linear_kernel()

indices = pd.Series(df.index)

# Visualizando se deu tudo certo (até o hotel 5)
indices[:5]

"""**Criando um sistema de recomendação usando a similaridade de coseno.**"""

def recomendacoes(name, cosine_similarities = cosine_similarities):
    
    # Lista vazia
    hoteis_recomendados = []
    
    # Índice do hotel que corresponde ao nome do hotel
    idx = indices[indices == name].index[0]

    # Criação de uma série com as pontuações de similaridade em ordem DESCRESCENTE
    score_series = pd.Series(cosine_similarities[idx]).sort_values(ascending = False)

    # Índices dos 10 hotéis mais semelhantes, exceto ele próprio 
    top_10_indexes = list(score_series.iloc[1:11].index)
    
    # Preenchimento da lista com os nomes dos 10 principais hotéis correspondentes
    for i in top_10_indexes:
        hoteis_recomendados.append(list(dataset.index)[i])
        
    return hoteis_recomendados

"""**Parâmetros obtidos no próprio Google.**"""

recomendacoes('Hilton Seattle Airport & Conference Center')

recomendacoes("The Bacon Mansion Bed and Breakfast")

"""# Conclusão:

*Código que pode ser usado em diversas aplicações WEB em variados nichos e que serviu para melhorar as técnicas de linguagem natural.*
"""

#Autor: Jonatas A. Liberato
#Ref: Eduardo Rocha